#*
 Copyright 2010-2014 Norconex Inc.
 
 This file is part of Norconex HTTP Collector.
 
Norconex HTTP Collector is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.
 
Norconex HTTP Collector is distributed in the hope that it will be useful, 
 but WITHOUT ANY WARRANTY; without even the implied warranty of 
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.
 
 You should have received a copy of the GNU General Public License
 along with Norconex HTTP Collector. If not, see <http://www.gnu.org/licenses/>.
*#
#set($h1 = '#')
#set($h2 = '##')
#set($h3 = '###')
#set($h4 = '####')
#set($arg = '<arg>')
<head><title>Features</title><meta name="Author" content="Norconex Inc." /></head> 


$h2 Features and Qualities

Have a look at what you can expect from this product.  Please 
[be vocal](https://github.com/Norconex/collector-http/issues)
about things you would like to see included in future releases.

*******************************************************************************

<h6>&nbsp;</h6>

  * **Portability**  
    Deploy your configuration to different 
    locations (staging, prod, etc) and environment specific settings
    (IPs, URLs, etc) are kept in separate, environment-specific variable files.
     
  * **Extensibility**  
    Almost every feature can be replaced by custom code
    or be extended.  

  * **Reusability**  
    Break your configuration the way you like in configuration
    fragments, and share entire configuration sections between many
    crawler instances.

  * **Ease of maintenance**  
    Only one central copy of the crawler binaries is
    required and/or needs to be updated.  No more duplication of binaries or 
    identical configuration values across several crawlers.

  * **Easy for developers to extend**  
    Many insertion points.  Basic Java, no custom framework to learn.
    Interfaces are clearly defined and to use your own
    implementation, you simply write your class name where appripriate in
    your configuration file.  The
    HTTP Collector is expected to be extended often by integrators.

  * **Supports different hit interval according to different schedules**  
    For long running crawls, if your web site customer traffic is high during 
    the day, but low at night or weekends, you can define different delays
    between each URL crawled according to different time periods.

  * **Easy for non-technical people to use**  
    While developers will see more benefits, the out-of-the-box features
    remain well-documented and easy to configure for non-developers.
    
  * **Good documentation**  
    Besides examples provided on this site or in distributed packages, 
    up-to-date Javadocs are automatically generated with every release, 
    and they constitute most of the documentation, each of them containing
    XML configuration options where applicable.
   
  * **Cross vendors**  
    Not tied to a specific search engine or data repository. You can swith 
    to another search vendor and still use your Norconex HTTP Collector
    installation.
  
  * **Powerful**  
    Can do more than just crawling.  Allows for any kinds of document 
    manipulation, before of after their text is extracted.
    
  * **Cross-platform**  
    100% Java-based.  As long as you can get Java on your operating system,
    you are good to go.  For instance, the same HTTP Collector installation 
    can run on Windows or Linux.
    
  * **Obtain and manipulate document metadata**  
    All metadata found in documents are extracted by default (both from 
    document properties and HTTP headers). 
    You can easily add, modify, remove, rename, etc.  
    
  * **Handles deleted documents**  
    The HTTP Collector will detect and perform appropriate action when a URL
    no longer exists (e.g. remove from search index).
    
  * **Robots.txt and in-page Robots meta tag support**  
    Can easily be turned off for your own sites.

<h6>&nbsp;</h6>

  * **sitemap.xml support**  
    Includes sitemap index, text or gzip.
    
  * **Embeddable**  
    Can be integrated and run from other java applications (while still
    using file-based configuration or not).

  * **Offers event listeners**  
    Offers listeners for most crawling events, allowing you to build 
    your own add-on solutions, such as custom crawling reports.

  * **Logs are meaningful and verbose**  
    Can log every document failure with exact cause of failure. Uses 
    [log4j](http://logging.apache.org/log4j/1.2/) to
    control log level.  

  * **Resumable upon failure**  
    Upon a major crash (e.g. server shutdown), you can resume the HTTP Collector
    and it will pick-up exactly where it left.  You can also stop it and
    resume it yourself.
    
  * **Can indicate progress**  
    Uses [Norconex JEF](http://www.norconex.com/product/jef) which 
    automatically stores it progress on file.  With the use of tools such as
    Norconex JEF Monitor, you can visualize the crawl progression.
    
  * **XML Configurable**  
    Can be configured via simple XML.
    
  * **Supports various website authentication schemes**  
    Supports FORM, BASIC, DIGEST, NTLM implementations out-of-the-box
    (with experimental support for SPNEGO and Kerberos).
    You can also plug your custom authentication logic.

  * **Broken down into reusable modules**  
    HTTP Collector uses 
    [Norconex Importer](http://www.norconex.com/product/importer) and 
    [Norconex Committer](http://www.norconex.com/product/committer)
    which can both be used on their own for different purposes (e.g. to build
    your own collectors).
    
  * **Supports cookies**  
    Humm... cookies!
    
  * **Relies on proven technology**  
    Many of HTTP Collector core features rely on proven technology such
    as Apacke Tika and Apache Derby.

  * **Detects modified documents**  
    Detect if documents have been modified or from previous run.
    
  * **Fast**  
    Can control the crawling speed.  Also offers to skip downloading of 
    unmodified documents, based on HTTP headers only (for subsequent runs).
    
  * **Multi-threaded**  
    Can use multiple threads to crawl web sites faster, while respecting 
    the minimum delay between each page crawling.
    
  * **Can normalize URLs**  
    Offers URL normalization to treat URL variations as one.

  * **Tested with millions of URLs**  
    Single or multiple crawler instances have been tested with millions 
    of web pages and documents without issues.
   
  * **Flexibility:**  
    There are more than one way to do something. 
    Experiment and have fun!

  * **And of course...**  
    Much more! 

