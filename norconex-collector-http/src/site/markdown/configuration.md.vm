#*
 Copyright 2010-2014 Norconex Inc.
 
 This file is part of Norconex HTTP Collector.
 
Norconex HTTP Collector is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.
 
Norconex HTTP Collector is distributed in the hope that it will be useful, 
 but WITHOUT ANY WARRANTY; without even the implied warranty of 
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.
 
 You should have received a copy of the GNU General Public License
 along with Norconex HTTP Collector. If not, see <http://www.gnu.org/licenses/>.
*#
#set($h1 = '#')
#set($h2 = '##')
#set($h3 = '###')
#set($h4 = '####')
<head><title>Configuration</title><meta name="Author" content="Norconex Inc." /></head> 

$h2 Configuration Quick start

To get started quickly, download the latest version of Norconex HTTP Collector
and locate the file [./examples/HOWTO_RUN_EXAMPLES.txt](./examples/HOWTO_RUN_EXAMPLES.txt).  This file will
point you to functional configuration files and will have you running a very
simple crawler in no time.  These sample files are also made available here
for your convinience:

  * [Functional Minimum Configuration Sample](./examples/minimum/minimum-config.xml)
  * [Functional Complex Configuration Sample](./examples/complex/complex-config.xml)
  * [Self-Documented Configuration Reference](./examples/collector-http-config-reference.xml)

$h2 HTTP Collector Configuration Options

To get the full potential of Norconex HTTP Collector and learn which parts 
can easily be extended, refer to the following for an XML-based configuration.
Entries with a "class" attribute are expecting an implementation
of your choice.   The HTTP Collector API offers several concrete
implementations already.  Configuration options with a default value
does not have to be defined.  Developers can also create their own
by implementing the proper Java interfaces. Refer to the
[Norconex HTTP Collector JavaDoc](./apidocs/index.html)
and/or see further down what interfaces you can implement.
Go to the
[Extend the HTTP Collector](./usage.html#Extend_the_HTTP_Collector)
page for more details on adding your own implementations.

    <httpcollector id="...">
    
      <progressDir>...</progressDir>
      <logsDir>...</logsDir>
    
      <crawlerDefaults>
        <startURLs>
          <url>...</url>
          <urlsFile>...</urlsFile>
          ...
        </startURLs>
        <userAgent>...</userAgent>
        <urlNormalizer class="..." />
        <delay class="..." />
        <numThreads>...</numThreads>
        <maxDepth>...</maxDepth>
        <maxDocuments>...</maxDocuments>
        <workDir>...</workDir>
        <keepDownloads>...</keepDownloads>
        <orphansStrategy>...</orphansStrategy>
        <crawlerListeners>
          <listener class="..."/>
          ...
        </crawlerListeners>
        <crawlDataStoreFactory class="" />
        <httpClientFactory class="..." />
        <referenceFilters>
          <filter class="..." />
          ...
        </referenceFilters>
        <robotsTxt ignore="..." class="..."/>
        <sitemap ignore="..." lenient="..." class="...">
          <location>...</location>
         ...
        </sitemap>
        <metaFetcher class="..." />
        <metadataFilters>
          <filter class="..." />
          ...
        </metadataFilters>
        <metadataChecksummer class="..." />
        <documentFetcher class="..." />
        <robotsMeta ignore="..." class="..."/>
        <linkExtractors>
          <extractor class="..." />
        </linkExtractors>
        <documentFilters>
          <filter class="..." />
          ...
        </documentFilters>
        <preImportProcessors>
          <processor class="..." />
          ...
        </preImportProcessors>
    
        <importer>
           <!-- refer to Importer documentation -->
        </importer>
    
        <documentChecksummer class="..." />
    
        <postImportProcessors>
          <processor class="..."></processor>
        </postImportProcessors>
    
        <committer class="..." />
      </crawlerDefaults>
    
      <crawlers>
        <crawler id="...">
           <!-- All default crawler options are the same you use can use 
                here (overwriting defaults).  You need to define at least one
                crawler. -->
        </crawler>
        ...
      </crawlers>
    
    </httpcollector>

The table below lists interface names that you can easily extend, and
also lists available out-of-the-box implementations.

In the configuration file, **you have to use the fully qualified name**,
as defined in the Javadoc.  Click on a class or interface name to go directly
to its full documentation, with extra configuration options.

When a default implementation exists for a configuration option taking
a "class" attribute, the default implementations is <u>underlined</u>. 

#set($httpAPI="./apidocs/com/norconex/collector/http")
#set($coreAPI="../collector-core/apidocs/com/norconex/collector/core")

<table>
 <thead>
  <tr>
   <th>Tag</th>
   <th>Description</th>
   <th>Classes</th>
   <th>Interface</th>
  </tr>
 </thead>
 <tbody>
   <tr>
     <td>httpcollector</td>
     <td>Root tag, you must give your configuration a unique identifier value.</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
   <tr>
     <td>progressDir</td>
     <td>Directory where to store crawling progress files.  Default is "./progress".</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
   <tr>
     <td>logsDir</td>
     <td>Directory where crawl logs will be stored.  Default is "./logs".</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
   <tr>
     <td>startURLs</td>
     <td>One or more &lt;url&gt; or files containing URls to start crawling from.  
         Try to use in combination with filters.</td>
     <td>N/A</td>
     <td>N/A</td>
   </tr>
    <tr>
     <td>userAgent</td>
     <td>The crawler "User-Agent" value to identify your crawler to sites you crawl.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>urlNormalizer</td>
     <td>Normalizes incoming URLs.</td>
     <td><a href="$httpAPI/url/impl/GenericURLNormalizer.html">GenericURLNormalizer</a></td>
     <td><a href="$httpAPI/url/IURLNormalizer.html">IURLNormalizer</a></td>
    </tr>
    <tr>
     <td>delay</td>
     <td>Handles interval between each page download.</td>
     <td><u><a href="$httpAPI/delay/impl/GenericDelayResolver.html">GenericDelayResolver</a></u></td>
     <td><a href="$httpAPI/delay/IDelayResolver.html">IDelayResolver</a></td>
    </tr>
    <tr>
     <td>numThreads</td>
     <td>Number of execution threads for a crawler.  Default is 2.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>maxDepth</td>
     <td>How many level deep to crawl from start URL(s).  Default is -1 (unlimited).</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>maxDocuments</td>
     <td>Maximum documents to successfully process.  Default is -1 (unlimited).</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>workDir</td>
     <td>Where to store files created as part of crawling activies.  Default is "./work".</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>keepDownloads</td>
     <td>Whether to keep downloaded files. Defaut is false.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>orphansStrategy</td>
     <td>What to do with urls not being referenced anymore. IGNORE (default), DELETE, or PROCESS.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
    <tr>
     <td>crawlerListeners</td>
     <td>Listen to crawling events.</td>
     <td></td>
     <td><a href="$coreAPI/crawler/event/ICrawlerEventListener.html">ICrawlerEventListener</a></td>
    </tr>
    <tr>
     <td>crawlDataStoreFactory</td>
     <td>URLs and crawl-related information data store.</td>
     <td>
       <u><a href="$coreAPI/data/store/impl/mapdb/MapDBCrawlDataStoreFactory.html">MapDBCrawlDataStoreFactory</a></u>,
       <a href="$httpAPI/data/store/impl/jdbc/JDBCCrawlDataStoreFactory.html">JDBCCrawlDataStoreFactory</a>,
       <a href="$httpAPI/data/store/impl/mongo/MongoCrawlDataStoreFactory.html">MongoCrawlDataStoreFactory</a>
     </td>
     <td><a href="$coreAPI/data/store/ICrawlDataStoreFactory.html">ICrawlDataStoreFactory</a></td>
    </tr>
    <tr>
     <td>httpClientFactory</td>
     <td>HTTP Client creation and initialization.</td>
     <td><u><a href="$httpAPI/client/impl/GenericHttpClientFactory.html">GenericHttpClientFactory</a></u></td>
     <td><a href="$httpAPI/client/IHttpClientFactory.html">IHttpClientFactory</a></td>
    </tr>
    <tr>
     <td>referenceFilters</td>
     <td>Filter based on refereences (i.e. URLs).</td>
     <td>
       <a href="$coreAPI/filter/impl/ExtensionReferenceFilter.html">ExtensionReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexReferenceFilter.html">RegexReferenceFilter</a>,
       <a href="$httpAPI/filter/impl/SegmentCountURLFilter.html">SegmentCountURLFilter</a>
     </td>
     <td><a href="$coreAPI/filter/IReferenceFilter.html">IReferenceFilter</a></td>
    </tr>
    <tr>
     <td>robotsTxt</td>
     <td>Handle robots.txt files.</td>
     <td><u><a href="$httpAPI/robot/impl/StandardRobotsTxtProvider.html">StandardRobotsTxtProvider</a></u></td>
     <td><a href="$httpAPI/robot/IRobotsTxtProvider.html">IRobotsTxtProvider</a></td>
    </tr>
    <tr>
     <td>sitemap</td>
     <td>Handle Sitemap files.</td>
     <td><u><a href="$httpAPI/sitemap/impl/StandardSitemapResolverFactory.html">StandardSitemapResolverFactory</a></u></td>
     <td><a href="$httpAPI/sitemap/ISitemapResolverFactory.html">ISitemapResolverFactory</a></td>
    </tr>
    <tr>
     <td>metaFetcher</td>
     <td>Fetches HTTP headers for a URL.</td>
     <td><a href="$httpAPI/fetch/impl/GenericMetadataFetcher.html">GenericMetadataFetcher</a></td>
     <td><a href="$httpAPI/fetch/IHttpMetadataFetcher.html">IHttpMetadataFetcher</a></td>
    </tr>
    <tr>
     <td>metadataFilters</td>
     <td>Filter based on HTTP Headers.</td>
     <td>
       <a href="$coreAPI/filter/impl/ExtensionReferenceFilter.html">ExtensionReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexReferenceFilter.html">RegexReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexMetadataFilter.html">RegexMetadataFilter</a>,
       <a href="$httpAPI/filter/impl/SegmentCountURLFilter.html">SegmentCountURLFilter</a>
     </td>
     <td><a href="$coreAPI/filter/IMetadataFilter.html">IMetadataFilter</a></td>
    </tr>
    <tr>
     <td>metadataChecksummer</td>
     <td>Create document checksum from HTTP Headers.</td>
     <td><u><a href="$httpAPI/checksum/impl/HttpMetadataChecksummer.html">HttpMetadataChecksummer</a></u></td>
     <td><a href="$coreAPI/checksum/IMetadataChecksummer.html">IMetadataChecksummer</a></td>
    </tr>
    <tr>
     <td>documentFetcher</td>
     <td>Fetch a document from URL.</td>
     <td><u><a href="$httpAPI/fetch/impl/GenericDocumentFetcher.html">GenericDocumentFetcher</a></u></td>
     <td><a href="$httpAPI/fetch/IHttpDocumentFetcher.html">IHttpDocumentFetcher</a></td>
    </tr>
    <tr>
     <td>robotsMeta</td>
     <td>Handle in-page robots instructions.</td>
     <td><u><a href="$httpAPI/robot/impl/StandardRobotsMetaProvider.html">StandardRobotsMetaProvider</a></u></td>
     <td><a href="$httpAPI/robot/IRobotsMetaProvider.html">IRobotsMetaProvider</a></td>
    </tr>
    <tr>
     <td>linkExtractors</td>
     <td>One or more &lt;extractor&gt; for extracting URLs and other link data
         from a document.</td>
     <td>
       <u><a href="$httpAPI/url/impl/HtmlLinkExtractor.html">HtmlLinkExtractor</a></u>,
       <a href="$httpAPI/url/impl/TikaLinkExtractor.html">TikaLinkExtractor.html</a>
     </td>
     <td><a href="$httpAPI/url/ILinkExtractor.html">ILinkExtractor</a></td>
    </tr>
    <tr>
     <td>documentFilters</td>
     <td>Filter documents.</td>
     <td>
       <a href="$coreAPI/filter/impl/ExtensionReferenceFilter.html">ExtensionReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexReferenceFilter.html">RegexReferenceFilter</a>,
       <a href="$coreAPI/filter/impl/RegexMetadataFilter.html">RegexMetadataFilter</a>,
       <a href="$httpAPI/filter/impl/SegmentCountURLFilter.html">SegmentCountURLFilter</a>
     </td>
     <td><a href="$coreAPI/filter/IDocumentFilter.html">IDocumentFilter</a></td>
    </tr>
    <tr>
     <td>preImportProcessors</td>
     <td>Process a document before import.</td>
     <td></td>
     <td><a href="$httpAPI/doc/IHttpDocumentProcessor.html">IHttpDocumentProcessor</a></td>
    </tr>
    <tr style="color: black;">
     <td><b>importer</b></td>
     <td>
       Performs document text extraction and manipulation.  It has many features
       and many 
       <a href="http://www.norconex.com/product/importer/formats.html">file formats</a> 
       are supported. Refer to 
       <a href="http://www.norconex.com/product/importer/configuration.html">Importer configuration options</a>.
     </td>
     <td></td>
     <td></td>
    </tr>
    <tr>
     <td>documentChecksummer</td>
     <td>Create a checksum from document.</td>
     <td><u><a href="$coreAPI/checksum/impl/MD5DocumentChecksummer.html">MD5DocumentChecksummer</a></u></td>
     <td><a href="$coreAPI/checksum/IDocumentChecksummer.html">IDocumentChecksummer</a></td>
    </tr>
    <tr>
     <td>postImportProcessors</td>
     <td>Process a document after import.</td>
     <td></td>
     <td><a href="$httpAPI/doc/IHttpDocumentProcessor.html">IHttpDocumentProcessor</a></td>
    </tr>
    <tr style="color: black;">
     <td><b>committer</b></td>
     <td>
       Where to commit a document when processed.  Different implementations are available. 
       Refer to <a href="http://www.norconex.com/product/committer">Committer module</a>.
     </td>
     <td></td>
     <td></td>
    </tr>
    <tr>
     <td>crawler</td>
     <td>Define as many crawlers as you like.  They must each have a unique identifier.</td>
     <td>N/A</td>
     <td>N/A</td>
    </tr>
 </tbody>
</table>

$h2 Importer Configuration Options

The [Importer](http://www.norconex.com/product/importer/) module
is an integral part of the HTTP Collector.  It is reponsible for extracting
text out of documents, and also provide document manipulation options and 
filtering options.  Much more is found in this module distributed with the HTTP Collector.   Read the 
[Importer Configuration Options](http://www.norconex.com/product/importer/configuration.html).

$h2 Committer Configuration Options

The [Committer](http://www.norconex.com/product/committer/) module is 
responsible for taking the text extracted out of 
your collected documents and submit it to your target repository (e.g. 
search engine).  Make sure you download a 
[Committer implementation](http://www.norconex.com/product/committer/#list)
matching your target repository.  Configuration options for is specific
to each committers.  Refer to your committer documentation.


$h2 More Options

There is a lot more you can do to structure your configuration files
the way you like.  Refer to this
[additional documentation](../commons-lang/apidocs/com/norconex/commons/lang/config/ConfigurationLoader.html)
for more configuration options such as creating reusable
configuration fragments and using variables to make your files easier
to maintain and more portable across different environments.

