The following are either things to be done, or ideas to consider:

- Test that sitemap resumes well if failing in-between.

- Document sitemap.

- Add Lucene-based CrawlDB implementation, offering disk or memory.

- Benchmark CrawlDB implementations and change default one if appropriate.

- Add optional flag to support gzip transfer (DecompressingHttpClient).

- Rotate/break log files when too big.

- If a URL filter rule was changed and a document is now rejected (never 
  processed), it will not be deleted (since it did not get a 404/NOT_FOUND.
  Maybe check if rejeceted URL in URLProcessor are in cache and send deletion 
  if so.

- Add more events to URL processing listener (one for each "step"?). 

- Integrate with distributed computing frameworks such as hadhoop.

- Test that IPV6 is supported

- Load test with million pages.

- Detect and follow RSS feeds (related to having multiple url extractors by
  content types).

- Offer option to have crawling rate adjust itself if a site is slow
  (in case it is the crawler hammering it).  Probably a change or new delay
  implementation... this means total download time (both for HEAD and GET) 
  should be added as document properties (not a bad idea to do regardless).

- Maybe provide additional URL Crawl store implementation.  Derby is default, 
  but should provide a non-embedded solution out-of-the-box, such as Mongo, or 
  Generic SQL-DB (hibernate?).

- To consider: Interface for Document Store for downloading documents.
  File system is used to down default, but could be 
  others like MongoDB?

- Consider having support for different URLExractors for different content 
  types.  Maybe also offer to default one ability to define custom patterns
  for how to identify URLs.

- Add option to skip certain http response code (like 500).  Those docs 
  that should not be added nor deleted because they are in a temporary bad 
  state.  Is it really useful?

  <!-- TODO: Add JEF Listeners -->
  <jef>
      <errorHandlers> 
         <handler class=""></handler>
      </errorHandlers>
      <jobProgressListener> 
         <listener class=""></listener>
      </jobProgressListener>
      <suiteLifeCycleListener> 
         <listener class=""></listener>
      </suiteLifeCycleListener>
  </jef>

- Create a FAQ:
  - How to follow redirects but not index page with a redirect?
  - How to ignore robot rules (both robots.txt and robots meta)?
  - How do I authenticate?