The following are either things to be done, or ideas to consider:


For 2.0.0:
============

- Distinguish between NEW and MODIFIED docs.

- Address Fix #17 where URL may change due to permanent redirect 
  (we want to store the final URL).  Create a new RedirectResolverStage 
  in DocumentPipeline?

- Create much better/complete unit tests using jetty with Junit to create
  crawls. 

- Cache Importer instance now created each time in ImportModuleStep?

- Change Committer to support readers instead of File???

- Write content to file if keeping download, else dispose

- remove content type and content family method to avoid
  conflicts with HTTP Collector?


Any release:
============

- When issuing a STOP, add an option to specify which crawler to stop,
  and let others run.
  
- Add a startup flag that will generate a batch/sh script for the user which 
  abstracts the call to the config file 
  (e.g., abcCollector.sh start|stop|resume)

- Add a command prompt action to flush the committer queue. This can be useful
  if the collector crashed for some reason, while there were files left in
  the committer queue.  Shall this be done on an AbstractCommitter class
  so all future collectors automatically has that?

- Add support for having different HTTPContext for each call and/or each
  sites.
  
- Add ability to provide custom URL pattern matching for URL Extraction
  (removing need to subclass).

- Ability to crawl up to a given size.  Absolute number or percentage of 
  disk capacity?

- Provide some duplicate detection mechanism.  Maybe using existing checksums.

- Add the ability to control how many successive crawls it has to go through
  before deleting a document.

- Make printing of debug messages for rejection easier/more obvious. Maybe
  have a logger interface with two implementation. One uses log4j as is, one
  uses inline xml configuration instead. 
  
- Have configurable the level of verbosity desired when logging exceptions.
  The options could be:
     - type: none|all|first|last
     - stacktrace: false|true

- When ready to make api changes, take out TargetURLRedirectStrategy and 
  put that logic into DefaultDocumentFetcher will require signature change.

- Use toSafeFileName and fromSafeFileName from classe FileUtil class to 
  ensure directories can be created without issues.

- Redo crawler event model so only 1 method needs to be implemented
  and an event object is passed, allowing any custom event implementation
  to be added, each event having a unique id.

- Consider BlockingQueue for Derby implementation providing fast memory access
  of queued URL, with the queue fed from DB by another thread. 

- Maybe: Add documentRejectedChecksum and documentRejectedHeadersChecksum to 
  IHttpCrawlerEventListener and fire them?  <-- Not if we redo event model.

- Make sure maxURL counter saved in progress is in sync with each time we commit
  to avoid having a counter higher than actual URLs processed in case of 
  failure (minor impact).

- Add optional flag to support gzip transfer (DecompressingHttpClient).

- Rotate/break log files when too big.

- If a URL filter rule was changed and a document is now rejected (never 
  processed), it will not be deleted (since it did not get a 404/NOT_FOUND).
  Maybe check if rejected URL in URLProcessor are in cache and send deletion 
  if so.

- Integrate with distributed computing frameworks such as hadhoop.

- Test that IPV6 is supported

- Consider having support for different URLExractors for different content 
  types.  Maybe also offer to default one ability to define custom patterns
  for how to identify URLs.

- Detect and follow RSS feeds (related to having multiple url extractors by
  content types).

- Offer option to have crawling rate adjust itself if a site is slow
  (in case it is the crawler hammering it).  Probably a change or new delay
  implementation... this means total download time (both for HEAD and GET) 
  should be added as document properties (not a bad idea to do regardless).

- To consider: Interface for Document Store for downloading documents.
  File system is used now, but could be others like MongoDB?

- Add option to skip certain http response code (like 500).  Those docs 
  that should not be added nor deleted because they are in a temporary bad 
  state.  Is it really useful?

MAYBE:
=======
- Create an XML schema/DTD?

- Start offering per-site options?  Like crawl interval and more?
  (can be achieve with defining different crawlers now).


  <!-- TODO: Add JEF Listeners -->
  <jef>
      <errorHandlers> 
         <handler class=""></handler>
      </errorHandlers>
      <jobProgressListener> 
         <listener class=""></listener>
      </jobProgressListener>
      <suiteLifeCycleListener> 
         <listener class=""></listener>
      </suiteLifeCycleListener>
  </jef>
